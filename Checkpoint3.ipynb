{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "np5vIrN6yHMC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets,transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "from torch import autograd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from higher import innerloop_ctx as metaloop\n",
        "\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veM3JbPyPrY2",
        "outputId": "f3433ec0-c53e-4fcd-8a7b-562f64ed1b3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique labels input [0 1 2 3 4 5 6 7 8 9], Count [5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n",
            "Unique labels after pruning [4 8], Count [  29 5851]\n",
            "Unique labels input [0 1 2 3 4 5 6 7 8 9], Count [ 980 1135 1032 1010  982  892  958 1028  974 1009]\n",
            "Unique labels after pruning [4 8], Count [982 974]\n",
            "Unique labels input [0 1 2 3 4 5 6 7 8 9], Count [ 980 1135 1032 1010  982  892  958 1028  974 1009]\n",
            "Unique labels after pruning [4 8], Count [982 974]\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "mnist_train = datasets.MNIST(root='./data', train=True, download=True)\n",
        "mnist_test = datasets.MNIST(root='./data', train=False, download=True)\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    '''returns an imbalanced mnist dataset depending on class_ratios and noise_ratios that you enter '''\n",
        "    def __init__(self, X, y, class_ratios, noise_ratios):\n",
        "        self.X = np.copy(X)\n",
        "        self.y = np.copy(y)\n",
        "        N = len(X)\n",
        "\n",
        "        #  get unique labels and their count in the dataset\n",
        "        self.unique_y, self.y_counts = np.unique(self.y, return_counts=True)\n",
        "        assert len(class_ratios) == len(self.unique_y)\n",
        "        assert len(noise_ratios) == len(self.unique_y)\n",
        "        print(f\"Unique labels input {self.unique_y}, Count {self.y_counts}\")\n",
        "\n",
        "        # normalize class ratio list\n",
        "        self.class_ratios = class_ratios\n",
        "\n",
        "        # prune dataset\n",
        "        for label in self.unique_y:\n",
        "          idx = np.where(self.y == label)[0]\n",
        "          n_samples = min(len(idx),int(self.class_ratios[label] * len(idx)))\n",
        "          to_remove = np.random.choice(idx, len(idx) - n_samples, replace=False)\n",
        "          self.y[to_remove] = -10000\n",
        "        self.X = self.X[self.y!=-10000]\n",
        "        self.y = self.y[self.y!=-10000]\n",
        "\n",
        "        self.unique_y, self.y_counts = np.unique(self.y, return_counts=True)\n",
        "        print(f\"Unique labels after pruning {self.unique_y}, Count {self.y_counts}\")\n",
        "\n",
        "        # add noise to each class\n",
        "        self.noise_ratios = noise_ratios\n",
        "        for label in self.unique_y:\n",
        "          idx = np.where(self.y == label)[0]\n",
        "          n_noise = min(len(idx),int(self.noise_ratios[label] * N))\n",
        "          to_noise = np.random.choice(idx, n_noise, replace=False)\n",
        "          self.y[to_noise] = np.random.choice(self.unique_y, n_noise, replace=True)\n",
        "\n",
        "        self.N_classes = len(self.unique_y)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.X[idx], self.y[idx]\n",
        "\n",
        "\n",
        "        onehot_label = np.where(self.unique_y == label)[0][0]\n",
        "        onehot_label = torch.tensor(onehot_label, dtype=torch.float32)\n",
        "\n",
        "        image = torch.tensor(image, dtype=torch.float32)/255.0\n",
        "        image = image.unsqueeze(0)\n",
        "\n",
        "        # image= self.transform(image)\n",
        "\n",
        "        return image, onehot_label\n",
        "\n",
        "\n",
        "batch_size = 64\n",
        "labels = mnist_train.targets.numpy()\n",
        "data = mnist_train.data.numpy()\n",
        "test_data = mnist_test.data.numpy()\n",
        "test_labels =  mnist_test.targets.numpy()\n",
        "class_ratios = {0:0, 1:0, 2:0, 3:0, 4:0.005, 5:0, 6:0, 7:0, 8:1, 9:0}\n",
        "noise_ratios = {0:0, 1:0, 2:0, 3:0, 4:0.1, 5:0, 6:0, 7:0, 8:0.1, 9:0}  # noise ratio should be < 1 for each class. If no noise 0\n",
        "\n",
        "class_ratios_test = {0:0, 1:0, 2:0, 3:0, 4:1, 5:0, 6:0, 7:0, 8:1, 9:0}\n",
        "noise_ratios_test = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}  # noise ratio should be < 1 for each class. If no noise 0\n",
        "train_loader = DataLoader(CustomDataset(data, labels, class_ratios,noise_ratios), batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(CustomDataset(test_data, test_labels, class_ratios_test,noise_ratios_test), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "val_loader = DataLoader(CustomDataset(test_data, test_labels, class_ratios_test,noise_ratios_test), batch_size=batch_size, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "aeS9vjhDTWGk",
        "outputId": "1ace4c9b-2414-413f-bf76-2e0ef6f6cdd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 1, 28, 28]) torch.Size([64])\n",
            "tensor(0.)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "for data, label in train_loader:\n",
        "  break\n",
        "print(data.shape, label.shape)\n",
        "\n",
        "plt.imshow(data[1,0])\n",
        "plt.colorbar()\n",
        "print(label[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pZNw1WdPur4",
        "outputId": "939b5fe2-96e5-4233-b426-47e74598719d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([100, 1, 28, 28])\n",
            "torch.Size([100])\n"
          ]
        }
      ],
      "source": [
        "# Collect 100 samples of the selected classes from test dataset as validation data\n",
        "X_validation = []\n",
        "y_validation = []\n",
        "\n",
        "for sample in val_loader:\n",
        "    X_validation.append(sample[0])\n",
        "    y_validation.append(sample[1])\n",
        "\n",
        "X_validation = torch.vstack(X_validation)[:100]\n",
        "y_validation = torch.hstack(y_validation)[:100]\n",
        "print(X_validation.shape)\n",
        "print(y_validation.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "MTuD04CVyY2H"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, n_out):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # convolutional layers\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        # fully connected layers\n",
        "        self.fc1 = nn.Linear(128 * 7 * 7, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64, n_out)\n",
        "\n",
        "        #max pooling layers\n",
        "        self.maxpool= nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.maxpool(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.maxpool(x)\n",
        "        x = x.view(-1, 128 * 7 * 7)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.fc4(x).squeeze()\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "WS9Lw2nHyslP"
      },
      "outputs": [],
      "source": [
        "class trainer():\n",
        "    def __init__(self, model, train_dataloader, test_dataloader, X_validation, y_validation, device):\n",
        "        self.device= device\n",
        "        self.model= model.to(device)\n",
        "        self.train_dataloader= train_dataloader\n",
        "        self.test_dataloader= test_dataloader\n",
        "        self.X_val= X_validation\n",
        "        self.y_val= y_validation\n",
        "\n",
        "\n",
        "    def train_normal(self, epochs):\n",
        "        '''trains the model using traditional approach'''\n",
        "        # criterion= torch.nn.CrossEntropyLoss()\n",
        "        criterion= torch.nn.BCEWithLogitsLoss().to(device)\n",
        "        optimizer= torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        running_loss_per_epoch=[]\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            total_num=0\n",
        "            running_loss= 0\n",
        "            train_acc=0\n",
        "            running_loss_per_batch= []\n",
        "            for i, data in enumerate(self.train_dataloader):\n",
        "                images, labels= data\n",
        "                images= images.to(self.device)\n",
        "                labels= labels.to(self.device)\n",
        "\n",
        "                predicted_logits= self.model(images)\n",
        "                pred_labels= (F.sigmoid(predicted_logits)>0.5).int()\n",
        "                loss= criterion(predicted_logits, labels.type_as(predicted_logits))\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "                train_acc += torch.sum(torch.eq(pred_labels, labels)).item()\n",
        "                running_loss_per_batch.append(loss.item())\n",
        "                total_num+= len(images)\n",
        "\n",
        "            running_loss_per_epoch.append(np.mean(running_loss_per_batch))\n",
        "            print(\"[epoch: %d] loss: %.3f    train accuracy: %.3f\" \\\n",
        "                    % (epoch + 1,  running_loss_per_epoch[-1], (train_acc/total_num)*100))\n",
        "            self.test()\n",
        "\n",
        "        return running_loss_per_epoch\n",
        "\n",
        "\n",
        "    def test(self):\n",
        "        criterion= torch.nn.BCEWithLogitsLoss().to(device)\n",
        "        self.model.eval()  #setting the model in evaluation mode\n",
        "        val_stat={}\n",
        "        all_accuracy=[]\n",
        "        all_loss=[]\n",
        "        all_predictions=[]\n",
        "        all_labels=[]\n",
        "        total_num=0\n",
        "        with torch.no_grad():\n",
        "            running_loss_per_batch=[]\n",
        "            for images, labels in self.test_dataloader:\n",
        "                images, labels= images.to(device), labels.to(device)\n",
        "                pred_logits = self.model(images).float()\n",
        "                labels_tensor = labels.clone().detach()\n",
        "                pred_labels= (F.sigmoid(pred_logits)>0.5).int()\n",
        "                loss=criterion(pred_logits, labels_tensor.type_as(pred_logits))\n",
        "                accuracy= torch.sum(torch.eq(pred_labels, labels)).item()\n",
        "                running_loss_per_batch.append(loss.item())    #tracking the loss, accuracy, predicted labels, and true labels\n",
        "                all_accuracy.append(accuracy)\n",
        "                all_predictions.append(pred_labels)\n",
        "                all_labels.append(labels)\n",
        "                total_num+=len(images)\n",
        "\n",
        "        val_stat['loss'] = np.mean(running_loss_per_batch)\n",
        "        val_stat['accuracy']=sum(all_accuracy)/total_num\n",
        "        val_stat['prediction']=torch.cat(all_predictions, dim=0)\n",
        "        val_stat['labels']=torch.cat(all_labels, dim=0)\n",
        "        print(f\"Test/Validation result: total sample: {total_num}, Avg loss: {val_stat['loss']:.3f}, Acc: {100*val_stat['accuracy']:.3f}%\")\n",
        "        return val_stat #returning the tracked values in the form of a dictionary\n",
        "\n",
        "\n",
        "\n",
        "    def train_reweighted(self, epochs):\n",
        "        '''trains the model using reweighted sampling approach'''\n",
        "        criterion = torch.nn.BCEWithLogitsLoss().to(self.device)\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        running_loss_per_epoch = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            self.model.train()\n",
        "            train_acc = 0\n",
        "            total_num = 0\n",
        "            running_loss = 0\n",
        "            running_loss_per_batch = []\n",
        "\n",
        "            for i, data in enumerate(self.train_dataloader):\n",
        "                # L2-L3: get data\n",
        "                images, labels = data\n",
        "                images, labels = images.to(self.device), labels.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                # Inner loop for reweighted training\n",
        "                with metaloop(self.model, optimizer) as (inner_model, inner_optimizer):\n",
        "                    # L4-L5: 1. forward pass to compute the initial weighted loss\n",
        "                    inner_train_outputs = inner_model(images)\n",
        "                    cost = criterion(inner_train_outputs, labels.type_as(inner_train_outputs))\n",
        "\n",
        "                    eps = torch.zeros(cost.size(), requires_grad=True, device=self.device)  # sample weights\n",
        "                    inner_trainloss = torch.sum(eps * cost)\n",
        "                    # L6-L7: model parameter update\n",
        "                    inner_optimizer.step(inner_trainloss)\n",
        "\n",
        "                    # L8-L10: computes validation loss and gradients for reweighting\n",
        "                    _criterion = torch.nn.BCEWithLogitsLoss(reduction= 'mean').to(self.device)\n",
        "                    val_loss = 0\n",
        "                    val_images, val_labels = self.X_val.to(device=self.device), self.y_val.to(device=self.device)\n",
        "                    val_pred_logits = inner_model(val_images)\n",
        "                    loss = _criterion(val_pred_logits, val_labels.type_as(val_pred_logits))\n",
        "                    eps_grads= (torch.autograd.grad(loss, eps, allow_unused=True)[0].detach())\n",
        "\n",
        "                # L11: Compute weights for reweighting based on validation gradients\n",
        "                w_tilde = torch.clamp(-eps_grads, min=0)\n",
        "                norm = torch.sum(w_tilde)\n",
        "                w = w_tilde / norm if norm != 0 else w_tilde\n",
        "\n",
        "                # L12-L14: Perform standard training step using reweighted loss\n",
        "                predicted_logits = self.model(images)\n",
        "                pred_labels = (F.sigmoid(predicted_logits) > 0.5).int()\n",
        "                criterion.reduction= 'none'\n",
        "                loss = criterion(predicted_logits, labels.type_as(predicted_logits))\n",
        "                loss = torch.sum(w * loss)\n",
        "\n",
        "                # Update variables\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                train_acc += torch.sum(torch.eq(pred_labels, labels)).item()\n",
        "                running_loss_per_batch.append(loss.item())\n",
        "                total_num += len(images)\n",
        "\n",
        "            running_loss_per_epoch.append(np.mean(running_loss_per_batch))\n",
        "            print(\"[epoch: %d] loss: %.3f    train accuracy: %.3f\" % (epoch + 1, running_loss_per_epoch[-1], (train_acc / total_num) * 100))\n",
        "            self.test()\n",
        "\n",
        "        return running_loss_per_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePMJeSa1yvz3",
        "outputId": "522acb0a-813e-4b29-c2aa-323c78648e27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[epoch: 1] loss: 0.694    train accuracy: 48.520\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.693, Acc: 50.204%\n",
            "[epoch: 2] loss: 0.693    train accuracy: 48.895\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.693, Acc: 49.796%\n",
            "[epoch: 3] loss: 0.693    train accuracy: 49.354\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.693, Acc: 50.204%\n",
            "[epoch: 4] loss: 0.693    train accuracy: 49.966\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.693, Acc: 50.204%\n",
            "[epoch: 5] loss: 0.693    train accuracy: 50.408\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.693, Acc: 50.204%\n",
            "[epoch: 6] loss: 0.693    train accuracy: 50.408\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.693, Acc: 50.204%\n",
            "[epoch: 7] loss: 0.693    train accuracy: 50.408\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.693, Acc: 50.204%\n",
            "[epoch: 8] loss: 0.693    train accuracy: 49.796\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.693, Acc: 50.204%\n",
            "[epoch: 9] loss: 0.693    train accuracy: 50.408\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.693, Acc: 50.204%\n",
            "[epoch: 10] loss: 0.693    train accuracy: 49.830\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.693, Acc: 50.204%\n",
            "[epoch: 11] loss: 0.693    train accuracy: 50.408\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.693, Acc: 50.204%\n",
            "[epoch: 12] loss: 0.693    train accuracy: 49.932\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.693, Acc: 50.204%\n",
            "[epoch: 13] loss: 0.693    train accuracy: 50.408\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.693, Acc: 50.204%\n",
            "[epoch: 14] loss: 0.693    train accuracy: 50.000\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.693, Acc: 50.204%\n",
            "[epoch: 15] loss: 0.693    train accuracy: 49.422\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.693, Acc: 50.204%\n",
            "[epoch: 16] loss: 0.693    train accuracy: 50.289\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.693, Acc: 50.204%\n",
            "[epoch: 17] loss: 0.693    train accuracy: 50.408\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.693, Acc: 50.204%\n",
            "[epoch: 18] loss: 0.693    train accuracy: 50.408\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.693, Acc: 50.204%\n",
            "[epoch: 19] loss: 0.693    train accuracy: 50.408\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.693, Acc: 50.204%\n",
            "[epoch: 20] loss: 0.693    train accuracy: 49.150\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.693, Acc: 49.796%\n"
          ]
        }
      ],
      "source": [
        "# Instantiate and train model using traditional method\n",
        "model = SimpleCNN(n_out=1)\n",
        "my_trainer_normal= trainer(model, train_loader, test_loader, X_validation, y_validation, device)\n",
        "normal_losses=my_trainer_normal.train_normal(epochs= 20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkseamgWzMdW",
        "outputId": "6427f8b4-b894-4765-ece5-d57d441ce234"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[epoch: 1] loss: 0.684    train accuracy: 50.680\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.697, Acc: 50.204%\n",
            "[epoch: 2] loss: 0.686    train accuracy: 50.408\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.697, Acc: 50.204%\n",
            "[epoch: 3] loss: 0.689    train accuracy: 50.748\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.677, Acc: 58.487%\n",
            "[epoch: 4] loss: 0.563    train accuracy: 50.935\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.458, Acc: 80.010%\n",
            "[epoch: 5] loss: 0.471    train accuracy: 50.680\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.315, Acc: 88.241%\n",
            "[epoch: 6] loss: 0.430    train accuracy: 50.374\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.251, Acc: 92.076%\n",
            "[epoch: 7] loss: 0.409    train accuracy: 50.510\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.202, Acc: 93.507%\n",
            "[epoch: 8] loss: 0.439    train accuracy: 50.663\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.219, Acc: 94.325%\n",
            "[epoch: 9] loss: 0.375    train accuracy: 50.612\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.155, Acc: 97.342%\n",
            "[epoch: 10] loss: 0.411    train accuracy: 49.949\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.107, Acc: 97.751%\n",
            "[epoch: 11] loss: 0.359    train accuracy: 49.711\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.105, Acc: 98.057%\n",
            "[epoch: 12] loss: 0.365    train accuracy: 49.949\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.099, Acc: 98.262%\n",
            "[epoch: 13] loss: 0.379    train accuracy: 50.238\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.094, Acc: 97.955%\n",
            "[epoch: 14] loss: 0.343    train accuracy: 50.102\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.077, Acc: 98.108%\n",
            "[epoch: 15] loss: 0.412    train accuracy: 50.170\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.112, Acc: 98.415%\n",
            "[epoch: 16] loss: 0.267    train accuracy: 50.544\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.060, Acc: 98.620%\n",
            "[epoch: 17] loss: 0.335    train accuracy: 50.476\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.070, Acc: 99.182%\n",
            "[epoch: 18] loss: 0.284    train accuracy: 50.408\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.048, Acc: 99.284%\n",
            "[epoch: 19] loss: 0.333    train accuracy: 50.731\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.074, Acc: 98.722%\n",
            "[epoch: 20] loss: 0.329    train accuracy: 50.731\n",
            "Test/Validation result: total sample: 1956, Avg loss: 0.056, Acc: 98.926%\n"
          ]
        }
      ],
      "source": [
        "# Instantiate and train model using proposed method\n",
        "model2= SimpleCNN(n_out=1)\n",
        "my_trainer2= trainer(model2, train_loader, test_loader, X_validation, y_validation, device)\n",
        "reweighted=my_trainer2.train_reweighted(epochs= 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wDZ2KWXHSf9G"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
